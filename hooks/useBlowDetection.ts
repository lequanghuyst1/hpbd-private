"use client";

import { useEffect, useRef, useState } from "react";

export function useBlowDetection(
  onBlowDetected: () => void,
  threshold: number = 0.5, // Ng∆∞·ª°ng √¢m l∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán th·ªïi
  sensitivity: number = 0.7, // ƒê·ªô nh·∫°y (0-1)
  canTrigger: () => boolean = () => true // Callback ƒë·ªÉ ki·ªÉm tra xem c√≥ ƒë∆∞·ª£c ph√©p trigger kh√¥ng
) {
  const [isListening, setIsListening] = useState(false);
  const [hasPermission, setHasPermission] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [blowProgress, setBlowProgress] = useState(0); // Ti·∫øn ƒë·ªô th·ªïi (0-100)
  const [permissionStatus, setPermissionStatus] = useState<
    "prompt" | "granted" | "denied" | "unknown"
  >("unknown");
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const dataArrayRef = useRef<Uint8Array | null>(null);
  const lastBlowTimeRef = useRef<number>(0);
  const progressRef = useRef<number>(0); // D√πng ref ƒë·ªÉ track progress
  const BLOW_COOLDOWN = 500; // Cooldown 500ms ƒë·ªÉ tr√°nh spam

  const startListening = async () => {
    try {
      setError(null);
      setIsLoading(true);

      // Ki·ªÉm tra secure context (HTTPS ho·∫∑c localhost)
      const isSecureContext =
        location.protocol === "https:" ||
        location.hostname === "localhost" ||
        location.hostname === "127.0.0.1" ||
        location.hostname === "[::1]";

      if (!isSecureContext) {
        // Khi truy c·∫≠p qua IP network (192.168.x.x), kh√¥ng ph·∫£i secure context
        // Tr√¨nh duy·ªát s·∫Ω kh√¥ng cho ph√©p truy c·∫≠p microphone
        throw new Error(
          `Microphone y√™u c·∫ßu HTTPS ho·∫∑c localhost ƒë·ªÉ ho·∫°t ƒë·ªông.\n\nB·∫°n ƒëang truy c·∫≠p qua: ${location.protocol}//${location.hostname}\n\nGi·∫£i ph√°p:\n1. Truy c·∫≠p qua localhost: http://localhost:3000\n2. Ho·∫∑c d√πng HTTPS (khi deploy)\n3. Ho·∫∑c c·∫•u h√¨nh HTTPS cho local development`
        );
      }

      // Ki·ªÉm tra xem tr√¨nh duy·ªát c√≥ h·ªó tr·ª£ getUserMedia kh√¥ng
      // H·ªó tr·ª£ c·∫£ API m·ªõi (mediaDevices.getUserMedia) v√† API c≈© (navigator.getUserMedia)
      const nav = navigator as any;
      const getUserMedia =
        navigator.mediaDevices?.getUserMedia ||
        nav.getUserMedia ||
        nav.webkitGetUserMedia ||
        nav.mozGetUserMedia;

      if (!getUserMedia) {
        throw new Error(
          "Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ microphone. Vui l√≤ng d√πng Chrome, Firefox ho·∫∑c Safari."
        );
      }

      // Y√™u c·∫ßu quy·ªÅn truy c·∫≠p microphone
      let stream: MediaStream;

      if (navigator.mediaDevices?.getUserMedia) {
        // S·ª≠ d·ª•ng API m·ªõi
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
          },
        });
      } else {
        // Fallback cho API c≈©
        stream = await new Promise<MediaStream>((resolve, reject) => {
          const oldGetUserMedia =
            (navigator as any).getUserMedia ||
            (navigator as any).webkitGetUserMedia ||
            (navigator as any).mozGetUserMedia;

          oldGetUserMedia.call(navigator, { audio: true }, resolve, reject);
        });
      }

      streamRef.current = stream;
      setHasPermission(true);
      setIsListening(true);
      setIsLoading(false);
      setError(null);

      // T·∫°o AudioContext
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      // T·∫°o AnalyserNode ƒë·ªÉ ph√¢n t√≠ch audio
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256; // K√≠ch th∆∞·ªõc FFT (Fast Fourier Transform)
      analyser.smoothingTimeConstant = 0.3;
      analyserRef.current = analyser;

      // K·∫øt n·ªëi microphone v·ªõi analyser
      const microphone = audioContext.createMediaStreamSource(stream);
      microphone.connect(analyser);
      microphoneRef.current = microphone;

      // T·∫°o m·∫£ng ƒë·ªÉ l∆∞u d·ªØ li·ªáu audio
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      dataArrayRef.current = dataArray as any;


      // H√†m ph√¢n t√≠ch audio li√™n t·ª•c v·ªõi thanh progress
      const PROGRESS_FRAMES_NEEDED = 40; // C·∫ßn 40 frame (kho·∫£ng 0.7s) ƒë·ªÉ ƒë·∫ßy thanh - nh·∫°y h∆°n cho mobile
      progressRef.current = 0; // Reset progress khi b·∫Øt ƒë·∫ßu

      const analyze = () => {
        if (!analyserRef.current || !dataArrayRef.current) return;

        analyserRef.current.getByteFrequencyData(dataArrayRef.current as any);

        // T√≠nh to√°n nƒÉng l∆∞·ª£ng ·ªü c√°c d·∫£i t·∫ßn s·ªë kh√°c nhau
        const bufferLength = dataArrayRef.current.length;

        // D·∫£i t·∫ßn s·ªë th·∫•p (20-200Hz) - ti·∫øng th·ªïi ch·ªß y·∫øu ·ªü ƒë√¢y
        let lowFreqSum = 0;
        const lowFreqBins = Math.min(15, bufferLength);
        for (let i = 0; i < lowFreqBins; i++) {
          lowFreqSum += dataArrayRef.current[i];
        }
        const lowFreqAvg = lowFreqSum / lowFreqBins / 255;

        // D·∫£i t·∫ßn s·ªë trung (200-1000Hz) - ti·∫øng ƒë·ªông th√¥ng th∆∞·ªùng c√≥ nhi·ªÅu ·ªü ƒë√¢y
        let midFreqSum = 0;
        const midFreqStart = lowFreqBins;
        const midFreqEnd = Math.min(lowFreqBins + 30, bufferLength);
        for (let i = midFreqStart; i < midFreqEnd; i++) {
          midFreqSum += dataArrayRef.current[i];
        }
        const midFreqAvg = midFreqSum / (midFreqEnd - midFreqStart) / 255;

        // D·∫£i t·∫ßn s·ªë cao (1000Hz+) - ti·∫øng ƒë·ªông s·∫Øc th∆∞·ªùng c√≥ nhi·ªÅu ·ªü ƒë√¢y
        let highFreqSum = 0;
        const highFreqStart = midFreqEnd;
        for (let i = highFreqStart; i < bufferLength; i++) {
          highFreqSum += dataArrayRef.current[i];
        }
        const highFreqAvg = highFreqSum / (bufferLength - highFreqStart) / 255;

        // ƒê·∫∑c ƒëi·ªÉm c·ªßa ti·∫øng th·ªïi (t·ªëi ∆∞u cho mobile):
        // 1. NƒÉng l∆∞·ª£ng cao ·ªü t·∫ßn s·ªë th·∫•p
        // 2. NƒÉng l∆∞·ª£ng th·∫•p ·ªü t·∫ßn s·ªë trung v√† cao (nh∆∞ng linh ho·∫°t h∆°n)
        // 3. Ng∆∞·ª°ng th·∫•p ƒë·ªÉ d·ªÖ ph√°t hi·ªán tr√™n mobile

        const isBlowPattern =
          lowFreqAvg > threshold * sensitivity * 0.5 && // 50% c·ªßa threshold*sensitivity - nh·∫°y h∆°n
          (lowFreqAvg > midFreqAvg * 1.1 || lowFreqAvg > 0.15) && // Linh ho·∫°t: ch·ªâ c·∫ßn 1.1x ho·∫∑c >15%
          (lowFreqAvg > highFreqAvg * 1.2 || lowFreqAvg > 0.15) && // Linh ho·∫°t: ch·ªâ c·∫ßn 1.2x ho·∫∑c >15%
          lowFreqAvg > 0.12; // 12% - ng∆∞·ª°ng th·∫•p cho mobile

        // T√≠nh progress d·ª±a tr√™n pattern
        if (isBlowPattern) {
          // TƒÉng progress khi c√≥ pattern
          progressRef.current = Math.min(
            100,
            progressRef.current + 100 / PROGRESS_FRAMES_NEEDED
          );
        } else {
          // Gi·∫£m progress khi kh√¥ng c√≥ pattern (decay ch·∫≠m ƒë·ªÉ gi·ªØ progress t·ªët h∆°n)
          progressRef.current = Math.max(0, progressRef.current - 1.5);
        }

        // C·∫≠p nh·∫≠t progress state
        setBlowProgress(progressRef.current);


        // Khi progress ƒë·∫°t 100%, trigger success (ch·ªâ khi ƒë∆∞·ª£c ph√©p)
        if (progressRef.current >= 100) {
          const now = Date.now();
          if (now - lastBlowTimeRef.current > BLOW_COOLDOWN && canTrigger()) {
            lastBlowTimeRef.current = now;
            progressRef.current = 0; // Reset progress
            setBlowProgress(0);
            onBlowDetected();
          } else if (!canTrigger()) {
            // N·∫øu ch∆∞a ƒë∆∞·ª£c ph√©p, reset progress nh∆∞ng kh√¥ng trigger
            progressRef.current = 0;
            setBlowProgress(0);
          }
        }

        animationFrameRef.current = requestAnimationFrame(analyze);
      };

      analyze();
    } catch (err: any) {
      setHasPermission(false);
      setIsListening(false);
      setIsLoading(false);

      // X·ª≠ l√Ω c√°c lo·∫°i l·ªói kh√°c nhau
      if (
        err.name === "NotAllowedError" ||
        err.name === "PermissionDeniedError"
      ) {
        setPermissionStatus("denied");
        setError(
          "B·∫°n ƒë√£ t·ª´ ch·ªëi quy·ªÅn truy c·∫≠p microphone. Vui l√≤ng click v√†o icon üîí ho·∫∑c üé§ ·ªü thanh ƒë·ªãa ch·ªâ tr√¨nh duy·ªát v√† cho ph√©p microphone, sau ƒë√≥ l√†m m·ªõi trang."
        );
      } else if (
        err.name === "NotFoundError" ||
        err.name === "DevicesNotFoundError"
      ) {
        setError(
          "Kh√¥ng t√¨m th·∫•y microphone. Vui l√≤ng ki·ªÉm tra thi·∫øt b·ªã c·ªßa b·∫°n."
        );
      } else if (
        err.name === "NotReadableError" ||
        err.name === "TrackStartError"
      ) {
        setError(
          "Microphone ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi ·ª©ng d·ª•ng kh√°c. Vui l√≤ng ƒë√≥ng ·ª©ng d·ª•ng ƒë√≥."
        );
      } else {
        setError(err.message || "L·ªói kh√¥ng x√°c ƒë·ªãnh khi truy c·∫≠p microphone.");
      }
    }
  };

  const stopListening = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    setIsListening(false);
  };

  useEffect(() => {
    return () => {
      stopListening();
    };
  }, []);

  return {
    startListening,
    stopListening,
    isListening,
    hasPermission,
    error,
    isLoading,
    permissionStatus,
    blowProgress, // Tr·∫£ v·ªÅ progress ƒë·ªÉ hi·ªÉn th·ªã UI
  };
}
